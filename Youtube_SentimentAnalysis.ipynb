{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e684df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up YouTube API key\n",
    "YOUTUBE_API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "if not YOUTUBE_API_KEY:\n",
    "    raise ValueError(\"Set YOUTUBE_API_KEY env variable before running.\")\n",
    "\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=YOUTUBE_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a33b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1: SEARCH VIDEOS \n",
    "def search_youtube(query: str, max_results: int = 50) -> List[str]:\n",
    "    video_ids = []\n",
    "    next_page_token = None\n",
    "    while len(video_ids) < max_results:\n",
    "        request = youtube.search().list(\n",
    "            q=query,\n",
    "            part=\"id\",\n",
    "            maxResults=min(50, max_results - len(video_ids)),\n",
    "            pageToken=next_page_token,\n",
    "            type=\"video\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "        video_ids.extend(item[\"id\"][\"videoId\"] for item in response[\"items\"])\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    return video_ids[:max_results]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2: GET TRANSCRIPTS AND COMMENTS\n",
    "def get_transcript(video_id: str) -> str:\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        return \" \".join([entry['text'] for entry in transcript])\n",
    "    except (TranscriptsDisabled, NoTranscriptFound):\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching transcript for {video_id}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def get_comments(video_id: str, max_comments: int = 100) -> List[str]:\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "    while len(comments) < max_comments:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=min(100, max_comments - len(comments)),\n",
    "            pageToken=next_page_token,\n",
    "            textFormat=\"plainText\"\n",
    "        )\n",
    "        response = request.execute()\n",
    "        comments.extend(item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"] for item in response[\"items\"])\n",
    "        next_page_token = response.get(\"nextPageToken\")\n",
    "        if not next_page_token:\n",
    "            break\n",
    "    return comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 3: BUILD DATASET\n",
    "def build_dataset(query: str, n_videos: int = 50) -> pd.DataFrame:\n",
    "    video_ids = search_youtube(query, n_videos)\n",
    "    data = []\n",
    "    for i, vid in enumerate(video_ids, 1):\n",
    "        print(f\"[{i}/{n_videos}] Fetching video ID: {vid}\")\n",
    "        transcript = get_transcript(vid)\n",
    "        comments = get_comments(vid, max_comments=100)\n",
    "        data.append({\n",
    "            \"video_id\": vid,\n",
    "            \"transcript\": transcript,\n",
    "            \"comments\": \" \".join(comments),\n",
    "            \"source\": query\n",
    "        })\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db10c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 4: SENTIMENT ANALYSIS\n",
    "sentiment_model = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def analyze_sentiment(text: str) -> str:\n",
    "    if not text.strip():\n",
    "        return \"NEUTRAL\"\n",
    "    # Truncate long texts for performance; adjust length as needed\n",
    "    try:\n",
    "        result = sentiment_model(text[:512])[0]\n",
    "        label = result['label']\n",
    "        # Convert to standardized label\n",
    "        if label == \"POSITIVE\":\n",
    "            return \"POSITIVE\"\n",
    "        elif label == \"NEGATIVE\":\n",
    "            return \"NEGATIVE\"\n",
    "        else:\n",
    "            return \"NEUTRAL\"\n",
    "    except Exception as e:\n",
    "        print(f\"Sentiment error: {e}\")\n",
    "        return \"NEUTRAL\"\n",
    "\n",
    "def analyze_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    print(\"Analyzing transcript sentiment...\")\n",
    "    df['transcript_sentiment'] = df['transcript'].apply(analyze_sentiment)\n",
    "    print(\"Analyzing comment sentiment...\")\n",
    "    df['comment_sentiment'] = df['comments'].apply(analyze_sentiment)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f462c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 5: EVALUATION\n",
    "def evaluate_sentiment(df: pd.DataFrame, col_pred: str, col_true: str):\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(df[col_true], df[col_pred], zero_division=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 6: CLUSTERING TO IDENTIFY TOPICS\n",
    "def cluster_texts(df: pd.DataFrame, text_column: str = \"comments\", n_clusters: int = 5):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    X = vectorizer.fit_transform(df[text_column].fillna(\"\"))\n",
    "\n",
    "    print(\"Clustering texts with KMeans...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    df['cluster'] = clusters\n",
    "\n",
    "    score = silhouette_score(X, clusters)\n",
    "    print(f\"Silhouette Score: {score:.3f}\")\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530181c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 7: MAIN EXECUTION\n",
    "if __name__ == \"__main__\":\n",
    "    QUERY = \"car advertisement\"\n",
    "    N_VIDEOS = 50\n",
    "\n",
    "    # Step 1: Build dataset from YouTube\n",
    "    df = build_dataset(QUERY, N_VIDEOS)\n",
    "\n",
    "    # Step 2: Analyze sentiment on transcripts and comments\n",
    "    df = analyze_dataset(df)\n",
    "\n",
    "    # Step 3: Evaluate comment sentiment using transcript sentiment as proxy \"ground truth\"\n",
    "    evaluate_sentiment(df, col_pred='comment_sentiment', col_true='transcript_sentiment')\n",
    "\n",
    "    # Step 4: Cluster comments to find dominant topics\n",
    "    df = cluster_texts(df, text_column=\"comments\", n_clusters=5)\n",
    "\n",
    "    # Step 5: Save results\n",
    "    output_file = \"youtube_car_ads_sentiment.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSaved results to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
